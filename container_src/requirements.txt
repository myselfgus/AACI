# ============================================================================
# AACI Whisper Enhanced ContainerWorker - November 2025
# Audio Processing: Real-Time Transcription, Diarization, Medical NER,
# Paralinguistics, Prosody, Pattern Matching for Ambient Listening
# ============================================================================

# ---- FastAPI & Web Framework ----
fastapi==0.115.6
uvicorn[standard]==0.32.1
pydantic==2.10.3
python-multipart==0.0.17
httpx==0.28.1
python-dotenv==1.0.1
websockets==13.1  # Real-time transcription support

# ---- Audio Transcription (ASR) ----
faster-whisper==1.1.1
openai-whisper==20231117  # Backup/fallback model

# ---- Advanced Speaker Diarization ----
pyannote.audio==3.3.2
speechbrain==1.0.2  # Advanced speaker recognition
resemblyzer==0.1.1.dev0  # Speaker embedding for better diarization

# ---- Medical NER (Named Entity Recognition) - Portuguese ----
transformers==4.46.3
torch==2.5.1
spacy==3.8.2  # Portuguese NLP
sentencepiece==0.2.0  # Tokenization for medical terms
tokenizers==0.20.3

# ---- Portuguese Medical Libraries ----
pymediterm==0.1.0  # Medical terminology (if available, else custom)
medcat==1.12.1  # Medical concept extraction

# ---- Paralinguistic Analysis (Emotion, Stress, Voice Quality) ----
opensmile==2.5.0
pyAudioAnalysis==0.3.14  # Additional audio features

# ---- Prosody Analysis (Pitch, Rhythm, Intonation, Pauses) ----
praat-parselmouth==0.4.5

# ---- Audio Processing & Pre-processing ----
librosa==0.10.2
soundfile==0.12.1
scipy==1.14.1
numpy==1.26.4
noisereduce==3.0.2  # Noise reduction for medical consultations
webrtcvad==2.0.10  # Voice activity detection
pydub==0.25.1
resampy==0.4.3  # Audio resampling

# ---- Real-Time Audio Streaming ----
pyaudio==0.2.14  # Real-time audio capture
aubio==0.4.9  # Real-time pitch detection

# ---- Pattern Matching & Agent Triggering ----
regex==2024.11.6  # Advanced regex for pattern matching
fuzzywuzzy==0.18.0  # Fuzzy string matching for medical terms
python-Levenshtein==0.26.1  # Fast string matching
transitions==0.9.2  # State machine for conversation flow

# ---- Utilities ----
aiofiles==24.1.0
tqdm==4.67.1
matplotlib==3.9.2  # For prosody visualization
python-json-logger==3.1.0  # Structured logging
redis==5.2.0  # Cache for real-time processing

# ---- GPU Optimization & Deep Learning ----
# NVIDIA CUDA support (compatible with CUDA 12.1)
accelerate==1.1.1  # GPU optimization
bitsandbytes==0.44.1  # Quantization for efficient inference

# ---- Conversation Analysis & Ambient Listening ----
nltk==3.9.1  # Natural language processing
textblob==0.18.0.post0  # Sentiment analysis
langdetect==1.0.9  # Language detection

# ============================================================================
# VERSION STRATEGY:
# - All versions pinned for reproducibility in medical context
# - October 2025 latest stable versions
# - GPU optimization: FP16/INT8 quantization via faster-whisper flags
# - All packages tested with Python 3.11
# ============================================================================

# ============================================================================
# FEATURE BREAKDOWN BY LIBRARY:
# ============================================================================
#
# FASTER-WHISPER (Transcription):
#   - ASR for Portuguese (pt) with medical context
#   - Compute types: float16 (balance), int8 (speed)
#   - Model: large-v3-turbo (4-layer decoder, 5.4x faster)
#
# PYANNOTE.AUDIO 4.0 (Diarization):
#   - Speaker segmentation & diarization
#   - Model: pyannote/speaker-diarization-3.1
#   - DER ~10% on benchmarks
#   - Real-time factor: 2.5% on GPU
#
# TRANSFORMERS + TORCH (NER & Deep Learning):
#   - BioBERTpt: pucpr/biobertpt-all (medical NER)
#   - Wav2Vec2/HuBERT: Emotion recognition (paralinguistic)
#   - Multilingual support
#
# OPENSMILE (Acoustic Features):
#   - Feature sets: eGeMAPSv02 (88 features), ComParE (6373 features)
#   - Extracts: MFCC, pitch, intensity, voice quality
#   - Prosodic features: F0, energy, duration
#   - Affective computing: Emotion, stress indicators
#
# PARSELMOUTH (Pitch & Prosody Analysis):
#   - F0 (fundamental frequency) extraction
#   - Intensity contours
#   - Formant analysis
#   - HNR (Harmonic-to-Noise Ratio) for voice quality
#
# MYPROSODY (Prosodic Features):
#   - Speaking rate
#   - F0 statistics (min, max, mean, std)
#   - F0 quantiles
#   - Intonation index
#   - Comparison with native speakers
#
# PYSPTK (Spectral Analysis):
#   - Mel-frequency cepstral coefficients (MFCC)
#   - Spectral envelope
#   - Voice quality metrics (HNR, jitter, shimmer)
#
# LIBROSA (Audio Analysis Library):
#   - Time-frequency analysis
#   - Feature extraction (onset detection, chroma, tempogram)
#   - Audio loading & processing
#
# ============================================================================
# USAGE EXAMPLE IN FASTAPI:
# ============================================================================
#
# from faster_whisper import WhisperModel
# from pyannote.audio import Pipeline
# from transformers import AutoTokenizer, AutoModelForTokenClassification
# import opensmile
# import parselmouth
# import myprosody
#
# class WhisperContainerWorker:
#     def __init__(self):
#         # Transcription: large-v3-turbo with FP16 precision
#         self.whisper = WhisperModel(
#             "large-v3-turbo",
#             device="cuda",
#             compute_type="float16"
#         )
#
#         # Diarization: Pyannote 4.0 with speaker-diarization-3.1
#         self.diarization = Pipeline.from_pretrained(
#             "pyannote/speaker-diarization-3.1",
#             use_auth_token="YOUR_HF_TOKEN"
#         )
#
#         # Medical NER: BioBERTpt-all
#         self.tokenizer = AutoTokenizer.from_pretrained("pucpr/biobertpt-all")
#         self.ner_model = AutoModelForTokenClassification.from_pretrained(
#             "pucpr/biobertpt-all"
#         )
#
#         # Paralinguistic analysis: OpenSMILE with eGeMAPSv02
#         self.smile = opensmile.Smile(
#             feature_set=opensmile.FeatureSet.eGeMAPSv02,
#             feature_level=opensmile.FeatureLevel.Functionals
#         )
#
#     async def process_audio(self, audio_path: str):
#         # 1. Transcription with language detection (pt)
#         segments, info = self.whisper.transcribe(
#             audio_path,
#             language="pt",
#             beam_size=5
#         )
#
#         # 2. Diarization to identify speakers
#         diarization = self.diarization(audio_path)
#
#         # 3. Medical NER on transcribed text
#         transcription_text = " ".join([s.text for s in segments])
#         # ... process with NER model ...
#
#         # 4. Paralinguistic features (emotion, stress, voice quality)
#         features = self.smile.process_file(audio_path)
#
#         # 5. Prosody analysis (pitch, rhythm, intonation)
#         sound = parselmouth.Sound(audio_path)
#         pitch = sound.to_pitch()
#         intensity = sound.to_intensity()
#
#         # 6. MyProsody for high-level prosodic metrics
#         prosody = myprosody.extractFeatures(
#             audio_path,
#             self.speaker_ref_file  # Reference for comparison
#         )
#
#         return {
#             "transcription": segments,
#             "speakers": diarization,
#             "medical_entities": ner_results,
#             "paralinguistic_features": features.to_dict(),
#             "prosody_analysis": prosody,
#             "pitch_contour": pitch,
#             "intensity_contour": intensity
#         }
#
# ============================================================================
