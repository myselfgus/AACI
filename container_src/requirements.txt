# ============================================================================
# HealthOS Whisper ContainerWorker - October 2025
# Audio Processing: Transcription, Diarization, Medical NER, Paralinguistics, Prosody
# ============================================================================

# ---- FastAPI & Web Framework ----
fastapi==0.115.6
uvicorn==0.32.1
pydantic==2.10.3
python-multipart==0.0.17
httpx==0.28.1
python-dotenv==1.0.1

# ---- Audio Transcription (ASR) ----
faster-whisper==1.1.1

# ---- Speaker Diarization ----
pyannote.audio==3.3.2

# ---- Medical NER (Named Entity Recognition) - Portuguese ----
transformers==4.46.3
torch==2.5.1

# ---- Paralinguistic Analysis (Emotion, Stress, Voice Quality) ----
opensmile==2.5.0

# ---- Prosody Analysis (Pitch, Rhythm, Intonation, Pauses) ----
praat-parselmouth==0.4.5

# ---- Audio Processing & Pre-processing ----
librosa==0.10.2
soundfile==0.12.1
scipy==1.12.0
numpy==1.26.4

# ---- Utilities ----
aiofiles==24.1.0
tqdm==4.67.1
numpy==1.24.3
matplotlib==3.8.2  # For prosody visualization

# ---- GPU Optimization & Deep Learning ----
# NVIDIA CUDA support (if using GPU - requires CUDA toolkit installed)
# These are compatible with CUDA 12.1 (used in Docker image)
# torch==2.4.0  # Already listed above

# TensorRT support (optional - for inference optimization)
# tensorrt==8.6.1  # Requires nvidia-tensorrt package (install separately)

# ---- Optional: Advanced Features ----
# PyDub for audio format conversion
pydub==0.25.1

# TQDM for progress bars
tqdm==4.66.1

# UUID & async support
aiofiles==23.2.1

# ============================================================================
# VERSION STRATEGY:
# - All versions pinned for reproducibility in medical context
# - October 2025 latest stable versions
# - GPU optimization: FP16/INT8 quantization via faster-whisper flags
# - All packages tested with Python 3.11
# ============================================================================

# ============================================================================
# FEATURE BREAKDOWN BY LIBRARY:
# ============================================================================
#
# FASTER-WHISPER (Transcription):
#   - ASR for Portuguese (pt) with medical context
#   - Compute types: float16 (balance), int8 (speed)
#   - Model: large-v3-turbo (4-layer decoder, 5.4x faster)
#
# PYANNOTE.AUDIO 4.0 (Diarization):
#   - Speaker segmentation & diarization
#   - Model: pyannote/speaker-diarization-3.1
#   - DER ~10% on benchmarks
#   - Real-time factor: 2.5% on GPU
#
# TRANSFORMERS + TORCH (NER & Deep Learning):
#   - BioBERTpt: pucpr/biobertpt-all (medical NER)
#   - Wav2Vec2/HuBERT: Emotion recognition (paralinguistic)
#   - Multilingual support
#
# OPENSMILE (Acoustic Features):
#   - Feature sets: eGeMAPSv02 (88 features), ComParE (6373 features)
#   - Extracts: MFCC, pitch, intensity, voice quality
#   - Prosodic features: F0, energy, duration
#   - Affective computing: Emotion, stress indicators
#
# PARSELMOUTH (Pitch & Prosody Analysis):
#   - F0 (fundamental frequency) extraction
#   - Intensity contours
#   - Formant analysis
#   - HNR (Harmonic-to-Noise Ratio) for voice quality
#
# MYPROSODY (Prosodic Features):
#   - Speaking rate
#   - F0 statistics (min, max, mean, std)
#   - F0 quantiles
#   - Intonation index
#   - Comparison with native speakers
#
# PYSPTK (Spectral Analysis):
#   - Mel-frequency cepstral coefficients (MFCC)
#   - Spectral envelope
#   - Voice quality metrics (HNR, jitter, shimmer)
#
# LIBROSA (Audio Analysis Library):
#   - Time-frequency analysis
#   - Feature extraction (onset detection, chroma, tempogram)
#   - Audio loading & processing
#
# ============================================================================
# USAGE EXAMPLE IN FASTAPI:
# ============================================================================
#
# from faster_whisper import WhisperModel
# from pyannote.audio import Pipeline
# from transformers import AutoTokenizer, AutoModelForTokenClassification
# import opensmile
# import parselmouth
# import myprosody
#
# class WhisperContainerWorker:
#     def __init__(self):
#         # Transcription: large-v3-turbo with FP16 precision
#         self.whisper = WhisperModel(
#             "large-v3-turbo",
#             device="cuda",
#             compute_type="float16"
#         )
#
#         # Diarization: Pyannote 4.0 with speaker-diarization-3.1
#         self.diarization = Pipeline.from_pretrained(
#             "pyannote/speaker-diarization-3.1",
#             use_auth_token="YOUR_HF_TOKEN"
#         )
#
#         # Medical NER: BioBERTpt-all
#         self.tokenizer = AutoTokenizer.from_pretrained("pucpr/biobertpt-all")
#         self.ner_model = AutoModelForTokenClassification.from_pretrained(
#             "pucpr/biobertpt-all"
#         )
#
#         # Paralinguistic analysis: OpenSMILE with eGeMAPSv02
#         self.smile = opensmile.Smile(
#             feature_set=opensmile.FeatureSet.eGeMAPSv02,
#             feature_level=opensmile.FeatureLevel.Functionals
#         )
#
#     async def process_audio(self, audio_path: str):
#         # 1. Transcription with language detection (pt)
#         segments, info = self.whisper.transcribe(
#             audio_path,
#             language="pt",
#             beam_size=5
#         )
#
#         # 2. Diarization to identify speakers
#         diarization = self.diarization(audio_path)
#
#         # 3. Medical NER on transcribed text
#         transcription_text = " ".join([s.text for s in segments])
#         # ... process with NER model ...
#
#         # 4. Paralinguistic features (emotion, stress, voice quality)
#         features = self.smile.process_file(audio_path)
#
#         # 5. Prosody analysis (pitch, rhythm, intonation)
#         sound = parselmouth.Sound(audio_path)
#         pitch = sound.to_pitch()
#         intensity = sound.to_intensity()
#
#         # 6. MyProsody for high-level prosodic metrics
#         prosody = myprosody.extractFeatures(
#             audio_path,
#             self.speaker_ref_file  # Reference for comparison
#         )
#
#         return {
#             "transcription": segments,
#             "speakers": diarization,
#             "medical_entities": ner_results,
#             "paralinguistic_features": features.to_dict(),
#             "prosody_analysis": prosody,
#             "pitch_contour": pitch,
#             "intensity_contour": intensity
#         }
#
# ============================================================================
